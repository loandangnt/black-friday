{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict future sales amount for specific User and Product Code based on last month sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach is to try plug the data into a model: a **linear regression** and an **xgboost regression**. Encode categorical data as above.\n",
    "Which error metric should I prefer?\n",
    "How to choose the best feature (feature selection)? Use the pipeline. If not good, try look at the data and do univariate analysis.\n",
    "Tune the model -> grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/dangloan/Documents/learning_analytics/project/'\n",
    "df = pd.read_csv(path + '3_data/black_friday/train.csv')\n",
    "df_test = pd.read_csv(path + '3_data/black_friday/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dangloan/miniconda3/envs/py/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers to make the model less sensitive to too large values. Thus, decrease RMSE\n",
    "cutoff_purchase = np.percentile(df['Purchase'], 99.9)  # 99.9 percentile\n",
    "df.ix[df['Purchase'] > cutoff_purchase, 'Purchase'] = cutoff_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_column_except_y = [col for col in df.columns if col not in []]\n",
    "column_y = ['Purchase']\n",
    "\n",
    "X = df[every_column_except_y]\n",
    "y = df[column_y]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster ProductID, UserID by Purchase mean and count\n",
    "def cluster(df,train,test, variable):\n",
    "    a = df.groupby(variable)['Purchase'].agg({'count','mean'}).reset_index()\n",
    "    count = variable + '_count'\n",
    "    mean = variable + '_mean'\n",
    "    cluster = variable + '_cluster'\n",
    "    a.columns =[variable,count,mean]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    b = scaler.fit_transform(a[[count,mean]])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=5)  \n",
    "    kmeans.fit(b)\n",
    "\n",
    "    b_df = pd.DataFrame(b)\n",
    "\n",
    "    b_df['cluster'] = kmeans.labels_\n",
    "    \n",
    "    arr1 = np.array(kmeans.cluster_centers_)\n",
    "    arr2 = np.array([0,1,2,3,4]).reshape((5,1))\n",
    "    centroids1 = np.concatenate((arr1,arr2), axis=1)\n",
    "    df_cen = pd.DataFrame(centroids1, columns=['count_centr','mean_centr','cluster']) \n",
    "    df_cen['cluster']=df_cen['cluster'].astype(int)\n",
    "    b_df = pd.merge(b_df,df_cen,on='cluster', how='left')\n",
    "    b_df = b_df[['count_centr','mean_centr']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    b_df = pd.concat([b_df.reset_index(drop=True), \n",
    "                      a[[variable]].reset_index(drop=True)],\n",
    "                     axis=1,ignore_index= True)\n",
    "\n",
    "    b_df.columns = [variable + '_count_centr', variable + '_mean_centr', variable]\n",
    "\n",
    "    train_result = train.merge(b_df,\n",
    "                           on=variable, how='left')\n",
    "    test_result = test.merge(b_df,\n",
    "                           on=variable, how='left')\n",
    "    return train_result, test_result\n",
    "\n",
    "X_train1, X_test1 = cluster(X_train,X_train,X_test,'Product_ID')\n",
    "X_train1, X_test1 = cluster(X_train,X_train1,X_test1,'User_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_stat, df):\n",
    "    # Compute counts for high cardinality artributes\n",
    "    Occ_stats = df_stat.groupby('Occupation')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Occ_stats.columns = ['Occupation','Occ_count','Occ_mean','Occ_std']\n",
    "\n",
    "    Cat1_stats = df_stat.groupby('Product_Category_1')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat1_stats.columns = ['Product_Category_1','Cat1_count','Cat1_mean','Cat1_std']\n",
    "\n",
    "    Cat2_stats = df_stat.groupby('Product_Category_2')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat2_stats.columns = ['Product_Category_2','Cat2_count','Cat2_mean','Cat2_std']\n",
    "\n",
    "    Cat3_stats = df_stat.groupby('Product_Category_3')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat3_stats.columns = ['Product_Category_3','Cat3_count','Cat3_mean','Cat3_std']\n",
    "    \n",
    "    ProdID_stats = df_stat.groupby('Product_ID')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    ProdID_stats.columns = ['Product_ID', 'ProdID_count','ProdID_mean','ProdID_std']\n",
    "    \n",
    "    UserID_stats = df_stat.groupby('User_ID')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    UserID_stats.columns = ['User_ID', 'UserID_count','UserID_mean','UserID_std']\n",
    "\n",
    "    df1 = df.merge(ProdID_stats,on='Product_ID', how='left')\\\n",
    "                    .merge(UserID_stats,on='User_ID', how='left')\\\n",
    "                    .merge(Occ_stats,on='Occupation', how='left')\\\n",
    "                    .merge(Cat1_stats,on='Product_Category_1', how='left')\\\n",
    "                    .merge(Cat2_stats,on='Product_Category_2', how='left')\\\n",
    "                    .merge(Cat3_stats,on='Product_Category_3', how='left')\n",
    "\n",
    "\n",
    "    # Fill missing data\n",
    "    df1.fillna(0,inplace=True)\n",
    "    \n",
    "    \n",
    "    df1['Stay_In_Current_City_Years'] = df1['Stay_In_Current_City_Years'].str.strip(\"+\")\n",
    "    df1['Stay_In_Current_City_Years'] = pd.to_numeric(df1['Stay_In_Current_City_Years'])\n",
    "    df1.loc[df1['Stay_In_Current_City_Years'] == 4, \"Stay_In_Current_City_Years\"] = 10\n",
    "\n",
    "    \n",
    "    # Transform age\n",
    "    age_map = { '0-17' : 17, \n",
    "                  '18-25' : 25, \n",
    "                  '26-35' : 35, \n",
    "                  '36-45' : 45, \n",
    "                  '46-50' : 50,\n",
    "                  '51-55' : 55,\n",
    "                  '55+' : 70}\n",
    "\n",
    "    df1['Age_mapped'] = df1['Age'].map(age_map)\n",
    "    \n",
    "    df1 = df1.drop(['Purchase','Age','Occupation','Product_Category_1',\n",
    "              'Product_Category_2','Product_Category_3',\n",
    "              'Product_ID','User_ID'], axis=1)\n",
    "\n",
    "    return df1\n",
    "\n",
    "X_train1 = clean(X_train,X_train1)\n",
    "X_test1 = clean(X_train,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one hot encoding for Gender and City_Category features\n",
    "onehot_cols = ['Gender','City_Category']\n",
    "ce_one_hot = ce.OneHotEncoder(cols = onehot_cols)\n",
    "X_train1 = ce_one_hot.fit_transform(X_train1)\n",
    "X_test1 = ce_one_hot.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train1)\n",
    "X_test_scaled = scaler.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender_1</th>\n",
       "      <th>Gender_2</th>\n",
       "      <th>City_Category_1</th>\n",
       "      <th>City_Category_2</th>\n",
       "      <th>City_Category_3</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_ID_count_centr</th>\n",
       "      <th>Product_ID_mean_centr</th>\n",
       "      <th>User_ID_count_centr</th>\n",
       "      <th>User_ID_mean_centr</th>\n",
       "      <th>ProdID_count</th>\n",
       "      <th>ProdID_mean</th>\n",
       "      <th>ProdID_std</th>\n",
       "      <th>UserID_count</th>\n",
       "      <th>UserID_mean</th>\n",
       "      <th>UserID_std</th>\n",
       "      <th>Occ_count</th>\n",
       "      <th>Occ_mean</th>\n",
       "      <th>Occ_std</th>\n",
       "      <th>Cat1_count</th>\n",
       "      <th>Cat1_mean</th>\n",
       "      <th>Cat1_std</th>\n",
       "      <th>Cat2_count</th>\n",
       "      <th>Cat2_mean</th>\n",
       "      <th>Cat2_std</th>\n",
       "      <th>Cat3_count</th>\n",
       "      <th>Cat3_mean</th>\n",
       "      <th>Cat3_std</th>\n",
       "      <th>Age_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.126282</td>\n",
       "      <td>1.534210</td>\n",
       "      <td>1.102939</td>\n",
       "      <td>-0.235298</td>\n",
       "      <td>154</td>\n",
       "      <td>11426.487013</td>\n",
       "      <td>4122.743918</td>\n",
       "      <td>251</td>\n",
       "      <td>8250.334661</td>\n",
       "      <td>4424.608157</td>\n",
       "      <td>47194</td>\n",
       "      <td>9434.817816</td>\n",
       "      <td>5094.620553</td>\n",
       "      <td>112330</td>\n",
       "      <td>13610.155684</td>\n",
       "      <td>4288.311900</td>\n",
       "      <td>30274.0</td>\n",
       "      <td>10352.674870</td>\n",
       "      <td>5593.418510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.170660</td>\n",
       "      <td>-0.131530</td>\n",
       "      <td>-0.376948</td>\n",
       "      <td>0.204489</td>\n",
       "      <td>516</td>\n",
       "      <td>6501.972868</td>\n",
       "      <td>1915.160776</td>\n",
       "      <td>58</td>\n",
       "      <td>11073.827586</td>\n",
       "      <td>5068.334870</td>\n",
       "      <td>25020</td>\n",
       "      <td>9797.216667</td>\n",
       "      <td>5136.888504</td>\n",
       "      <td>120631</td>\n",
       "      <td>6236.131417</td>\n",
       "      <td>1910.299020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385089</td>\n",
       "      <td>-0.154557</td>\n",
       "      <td>3.488426</td>\n",
       "      <td>-0.561842</td>\n",
       "      <td>29</td>\n",
       "      <td>6124.655172</td>\n",
       "      <td>2358.975478</td>\n",
       "      <td>370</td>\n",
       "      <td>9340.305405</td>\n",
       "      <td>4911.578884</td>\n",
       "      <td>9807</td>\n",
       "      <td>9343.996941</td>\n",
       "      <td>4996.781994</td>\n",
       "      <td>91126</td>\n",
       "      <td>7499.600641</td>\n",
       "      <td>2013.649156</td>\n",
       "      <td>43897.0</td>\n",
       "      <td>7092.815432</td>\n",
       "      <td>3235.732789</td>\n",
       "      <td>13269.0</td>\n",
       "      <td>11761.739166</td>\n",
       "      <td>5081.664057</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.361595</td>\n",
       "      <td>1.221421</td>\n",
       "      <td>-0.376948</td>\n",
       "      <td>0.204489</td>\n",
       "      <td>652</td>\n",
       "      <td>8107.062883</td>\n",
       "      <td>2108.182733</td>\n",
       "      <td>100</td>\n",
       "      <td>10531.220000</td>\n",
       "      <td>5011.731054</td>\n",
       "      <td>57816</td>\n",
       "      <td>9229.586170</td>\n",
       "      <td>5054.295142</td>\n",
       "      <td>91126</td>\n",
       "      <td>7499.600641</td>\n",
       "      <td>2013.649156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.126282</td>\n",
       "      <td>1.534210</td>\n",
       "      <td>-0.376948</td>\n",
       "      <td>0.204489</td>\n",
       "      <td>124</td>\n",
       "      <td>13225.483871</td>\n",
       "      <td>3291.215263</td>\n",
       "      <td>83</td>\n",
       "      <td>10451.783133</td>\n",
       "      <td>5194.145022</td>\n",
       "      <td>57816</td>\n",
       "      <td>9229.586170</td>\n",
       "      <td>5054.295142</td>\n",
       "      <td>112330</td>\n",
       "      <td>13610.155684</td>\n",
       "      <td>4288.311900</td>\n",
       "      <td>51308.0</td>\n",
       "      <td>10260.907246</td>\n",
       "      <td>5309.731382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender_1  Gender_2  City_Category_1  City_Category_2  City_Category_3  \\\n",
       "0         1         0                1                0                0   \n",
       "1         1         0                0                1                0   \n",
       "2         1         0                1                0                0   \n",
       "3         1         0                1                0                0   \n",
       "4         1         0                1                0                0   \n",
       "\n",
       "   Stay_In_Current_City_Years  Marital_Status  Product_ID_count_centr  \\\n",
       "0                           3               1               -0.126282   \n",
       "1                          10               0                1.170660   \n",
       "2                           1               0               -0.385089   \n",
       "3                           2               0                3.361595   \n",
       "4                           1               0               -0.126282   \n",
       "\n",
       "   Product_ID_mean_centr  User_ID_count_centr  User_ID_mean_centr  \\\n",
       "0               1.534210             1.102939           -0.235298   \n",
       "1              -0.131530            -0.376948            0.204489   \n",
       "2              -0.154557             3.488426           -0.561842   \n",
       "3               1.221421            -0.376948            0.204489   \n",
       "4               1.534210            -0.376948            0.204489   \n",
       "\n",
       "   ProdID_count   ProdID_mean   ProdID_std  UserID_count   UserID_mean  \\\n",
       "0           154  11426.487013  4122.743918           251   8250.334661   \n",
       "1           516   6501.972868  1915.160776            58  11073.827586   \n",
       "2            29   6124.655172  2358.975478           370   9340.305405   \n",
       "3           652   8107.062883  2108.182733           100  10531.220000   \n",
       "4           124  13225.483871  3291.215263            83  10451.783133   \n",
       "\n",
       "    UserID_std  Occ_count     Occ_mean      Occ_std  Cat1_count     Cat1_mean  \\\n",
       "0  4424.608157      47194  9434.817816  5094.620553      112330  13610.155684   \n",
       "1  5068.334870      25020  9797.216667  5136.888504      120631   6236.131417   \n",
       "2  4911.578884       9807  9343.996941  4996.781994       91126   7499.600641   \n",
       "3  5011.731054      57816  9229.586170  5054.295142       91126   7499.600641   \n",
       "4  5194.145022      57816  9229.586170  5054.295142      112330  13610.155684   \n",
       "\n",
       "      Cat1_std  Cat2_count     Cat2_mean     Cat2_std  Cat3_count  \\\n",
       "0  4288.311900     30274.0  10352.674870  5593.418510         0.0   \n",
       "1  1910.299020         0.0      0.000000     0.000000         0.0   \n",
       "2  2013.649156     43897.0   7092.815432  3235.732789     13269.0   \n",
       "3  2013.649156         0.0      0.000000     0.000000         0.0   \n",
       "4  4288.311900     51308.0  10260.907246  5309.731382         0.0   \n",
       "\n",
       "      Cat3_mean     Cat3_std  Age_mapped  \n",
       "0      0.000000     0.000000          55  \n",
       "1      0.000000     0.000000          25  \n",
       "2  11761.739166  5081.664057          35  \n",
       "3      0.000000     0.000000          25  \n",
       "4      0.000000     0.000000          25  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error\n",
      "0.7441715936459665\n",
      "Test error\n",
      "0.7312699218365817\n",
      "R_squared\n",
      "0.6351130667302562\n",
      "Mean_squared_error\n",
      "2598.360241520075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Predict\n",
    "predictions = lr_model.predict(X_test_scaled)\n",
    "\n",
    "#Evaluate\n",
    "print ('Training error')\n",
    "print (lr_model.score(X_train_scaled, y_train))\n",
    "\n",
    "print ('Test error')\n",
    "print (lr_model.score(X_test_scaled, y_test))\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "print(\"R_squared\")\n",
    "print(r2_score(predictions, y_test))\n",
    "\n",
    "mse = mean_squared_error(predictions,y_test)\n",
    "print('Mean_squared_error')\n",
    "print(np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBRegressor(colsample_bytree=0.8,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.15,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1,\n",
    "                 n_estimators=15000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42)\n",
    "\n",
    "model.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "#Evaluate\n",
    "print (model.score(X_train_scaled, y_train))\n",
    "print (model.score(X_test_scaled, y_test))\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "print(r2_score(predictions, y_test))\n",
    "mse = mean_squared_error(predictions,y_test)\n",
    "print(np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,1)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgboost.XGBRegressor(learning_rate =0.1, n_estimators=1000,\n",
    "                                                         gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                         nthread=4, scale_pos_weight=1, seed=27), \n",
    "param_grid = param_test1, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(X_train_scaled,y_train)\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'n_estimators':range(100,20000,200)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgboost.XGBRegressor(learning_rate =0.1,max_depth=3, min_child_weight=1,\n",
    "                                                         gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                         nthread=4, scale_pos_weight=1, seed=27), \n",
    "param_grid = param_test1, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(X_train_scaled,y_train)\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster ProductID, UserID by Purchase mean and count\n",
    "def cluster(df,train, test, variable):\n",
    "    a = df.groupby(variable)['Purchase'].agg({'count','mean'}).reset_index()\n",
    "    count = variable + '_count'\n",
    "    mean = variable + '_mean'\n",
    "    cluster = variable + '_cluster'\n",
    "    a.columns =[variable,count,mean]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    b = scaler.fit_transform(a[[count,mean]])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=5)  \n",
    "    kmeans.fit(b)\n",
    "\n",
    "    b_df = pd.DataFrame(b)\n",
    "\n",
    "    b_df['cluster'] = kmeans.labels_\n",
    "    b_df = pd.concat([b_df.reset_index(drop=True), \n",
    "                      a[[variable]].reset_index(drop=True)],\n",
    "                     axis=1,ignore_index= True)\n",
    "\n",
    "    b_df.columns = [count,mean,cluster, variable]\n",
    "\n",
    "    train_result = train.merge(b_df[[variable, cluster]],\n",
    "                           on=variable, how='left')\n",
    "    test_result = test.merge(b_df[[variable, cluster]],\n",
    "                           on=variable, how='left')\n",
    "    return train_result, test_result\n",
    "\n",
    "X_train1, X_test1 = cluster(df,df,df_test,'Product_ID')\n",
    "X_train1, X_test1 = cluster(df,X_train1,X_test1,'User_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_stat, df):\n",
    "    #Compute counts for high cardinality artributes\n",
    "    Occ_stats = df_stat.groupby('Occupation')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Occ_stats.columns = ['Occupation','Occ_count','Occ_mean','Occ_std']\n",
    "\n",
    "    Cat1_stats = df_stat.groupby('Product_Category_1')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat1_stats.columns = ['Product_Category_1','Cat1_count','Cat1_mean','Cat1_std']\n",
    "\n",
    "    Cat2_stats = df_stat.groupby('Product_Category_2')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat2_stats.columns = ['Product_Category_2','Cat2_count','Cat2_mean','Cat2_std']\n",
    "\n",
    "    Cat3_stats = df_stat.groupby('Product_Category_3')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat3_stats.columns = ['Product_Category_3','Cat3_count','Cat3_mean','Cat3_std']\n",
    "    \n",
    "    ProdID_stats = df_stat.groupby('Product_ID')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    ProdID_stats.columns = ['Product_ID', 'ProdID_count','ProdID_mean','ProdID_std']\n",
    "    \n",
    "    UserID_stats = df_stat.groupby('User_ID')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    UserID_stats.columns = ['User_ID', 'UserID_count','UserID_mean','UserID_std']\n",
    "\n",
    "    df1 = df.merge(ProdID_stats,on='Product_ID', how='left')\\\n",
    "                    .merge(UserID_stats,on='User_ID', how='left')\\\n",
    "                    .merge(Occ_stats,on='Occupation', how='left')\\\n",
    "                    .merge(Cat1_stats,on='Product_Category_1', how='left')\\\n",
    "                    .merge(Cat2_stats,on='Product_Category_2', how='left')\\\n",
    "                    .merge(Cat3_stats,on='Product_Category_3', how='left')\n",
    "\n",
    "\n",
    "#    #Taking care of missing data\n",
    "#     cols = ['Occ_count','Cat1_count',\n",
    "#             'Cat2_count','Cat3_count']\n",
    "#     from sklearn.preprocessing import Imputer\n",
    "#     imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "#     imputer = imputer.fit(df1[cols])\n",
    "#     df1[cols] = imputer.transform(df1[cols])\n",
    "#     df1[cols] = df1[cols].fillna(0)\n",
    "    df1.fillna(0,inplace=True)\n",
    "    \n",
    "    \n",
    "    df1['Stay_In_Current_City_Years'] = df1['Stay_In_Current_City_Years'].str.strip(\"+\")\n",
    "    df1['Stay_In_Current_City_Years'] = pd.to_numeric(df1['Stay_In_Current_City_Years'])\n",
    "    df1.loc[df1['Stay_In_Current_City_Years'] == 4, \"Stay_In_Current_City_Years\"] = 10\n",
    "\n",
    "    \n",
    "    #transform age\n",
    "    age_map = { '0-17' : 17, \n",
    "                  '18-25' : 25, \n",
    "                  '26-35' : 35, \n",
    "                  '36-45' : 45, \n",
    "                  '46-50' : 50,\n",
    "                  '51-55' : 55,\n",
    "                  '55+' : 70}\n",
    "\n",
    "    df1['Age_mapped'] = df1['Age'].map(age_map)\n",
    "    \n",
    "    df1 = df1.drop(['Age','Occupation','Product_Category_1',\n",
    "              'Product_Category_2','Product_Category_3',\n",
    "            'Product_ID','User_ID'], axis=1)\n",
    "\n",
    "    return df1\n",
    "\n",
    "# X_train1 = clean(df,X_train1)\n",
    "X_test1 = clean(df,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode cat features\n",
    "onehot_cols = ['Gender','City_Category']\n",
    "X_test1 = ce_one_hot.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "X_test_scaled = scaler.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Purchase'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_test[['User_ID','Product_ID','Purchase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('/Users/dangloan/Documents/learning_analytics/project/3_data/black_friday/sample8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importances\n",
    "importances = model.feature_importances_\n",
    "# importances = model1.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [X_test1.columns[i] for i in indices]\n",
    "\n",
    "\n",
    "a=list(zip(names,importances[indices]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(X_test1.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(X_test1.shape[1]), names, rotation=75)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
