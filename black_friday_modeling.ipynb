{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict future sales amount for specific User and Product Code based on last month sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach is to try plug the data into a model: a **linear regression** and an **xgboost regression**. Encode categorical data as above.\n",
    "Which error metric should I prefer?\n",
    "How to choose the best feature (feature selection)? Use the pipeline. If not good, try look at the data and do univariate analysis.\n",
    "Tune the model -> grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/dangloan/Documents/learning_analytics/project/'\n",
    "df = pd.read_csv(path + '3_data/black_friday/train.csv')\n",
    "df_test = pd.read_csv(path + '3_data/black_friday/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers to make the model less sensitive to too large values. Thus, decrease RMSE\n",
    "cutoff_purchase = np.percentile(df['Purchase'], 99.9)  # 99.9 percentile\n",
    "df.ix[df['Purchase'] > cutoff_purchase, 'Purchase'] = cutoff_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_column_except_y = [col for col in df.columns if col not in []]\n",
    "column_y = ['Purchase']\n",
    "\n",
    "X = df[every_column_except_y]\n",
    "y = df[column_y]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster ProductID, UserID by Purchase mean and count\n",
    "def cluster(df,train,test, variable):\n",
    "    a = df.groupby(variable)['Purchase'].agg({'count','mean'}).reset_index()\n",
    "    count = variable + '_count'\n",
    "    mean = variable + '_mean'\n",
    "    cluster = variable + '_cluster'\n",
    "    a.columns =[variable,count,mean]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    b = scaler.fit_transform(a[[count,mean]])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=5)  \n",
    "    kmeans.fit(b)\n",
    "\n",
    "    b_df = pd.DataFrame(b)\n",
    "\n",
    "    b_df['cluster'] = kmeans.labels_\n",
    "    b_df = pd.concat([b_df.reset_index(drop=True), \n",
    "                      a[[variable]].reset_index(drop=True)],\n",
    "                     axis=1,ignore_index= True)\n",
    "\n",
    "    b_df.columns = [count,mean,cluster, variable]\n",
    "\n",
    "    train_result = train.merge(b_df[[variable, cluster]],\n",
    "                           on=variable, how='left')\n",
    "    test_result = test.merge(b_df[[variable, cluster]],\n",
    "                           on=variable, how='left')\n",
    "    return train_result, test_result\n",
    "\n",
    "X_train1, X_test1 = cluster(X_train,X_train,X_test,'Product_ID')\n",
    "X_train1, X_test1 = cluster(X_train,X_train1,X_test1,'User_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_stat, df):\n",
    "    # Compute counts for high cardinality artributes\n",
    "    Occ_stats = df_stat.groupby('Occupation')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Occ_stats.columns = ['Occupation','Occ_count','Occ_mean','Occ_std']\n",
    "\n",
    "    Cat1_stats = df_stat.groupby('Product_Category_1')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat1_stats.columns = ['Product_Category_1','Cat1_count','Cat1_mean','Cat1_std']\n",
    "\n",
    "    Cat2_stats = df_stat.groupby('Product_Category_2')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat2_stats.columns = ['Product_Category_2','Cat2_count','Cat2_mean','Cat2_std']\n",
    "\n",
    "    Cat3_stats = df_stat.groupby('Product_Category_3')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat3_stats.columns = ['Product_Category_3','Cat3_count','Cat3_mean','Cat3_std']\n",
    "    \n",
    "    ProdID_stats = df_stat.groupby('Product_ID')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    ProdID_stats.columns = ['Product_ID', 'ProdID_count','ProdID_mean','ProdID_std']\n",
    "    \n",
    "    UserID_stats = df_stat.groupby('User_ID')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    UserID_stats.columns = ['User_ID', 'UserID_count','UserID_mean','UserID_std']\n",
    "\n",
    "    df1 = df.merge(ProdID_stats,on='Product_ID', how='left')\\\n",
    "                    .merge(UserID_stats,on='User_ID', how='left')\\\n",
    "                    .merge(Occ_stats,on='Occupation', how='left')\\\n",
    "                    .merge(Cat1_stats,on='Product_Category_1', how='left')\\\n",
    "                    .merge(Cat2_stats,on='Product_Category_2', how='left')\\\n",
    "                    .merge(Cat3_stats,on='Product_Category_3', how='left')\n",
    "\n",
    "\n",
    "    # Fill missing data\n",
    "    df1.fillna(0,inplace=True)\n",
    "    \n",
    "    \n",
    "    df1['Stay_In_Current_City_Years'] = df1['Stay_In_Current_City_Years'].str.strip(\"+\")\n",
    "    df1['Stay_In_Current_City_Years'] = pd.to_numeric(df1['Stay_In_Current_City_Years'])\n",
    "    df1.loc[df1['Stay_In_Current_City_Years'] == 4, \"Stay_In_Current_City_Years\"] = 10\n",
    "\n",
    "    \n",
    "    # Transform age\n",
    "    age_map = { '0-17' : 17, \n",
    "                  '18-25' : 25, \n",
    "                  '26-35' : 35, \n",
    "                  '36-45' : 45, \n",
    "                  '46-50' : 50,\n",
    "                  '51-55' : 55,\n",
    "                  '55+' : 70}\n",
    "\n",
    "    df1['Age_mapped'] = df1['Age'].map(age_map)\n",
    "    \n",
    "    df1 = df1.drop(['Purchase','Age','Occupation','Product_Category_1',\n",
    "              'Product_Category_2','Product_Category_3',\n",
    "              'Product_ID','User_ID'], axis=1)\n",
    "\n",
    "    return df1\n",
    "\n",
    "X_train1 = clean(X_train,X_train1)\n",
    "X_test1 = clean(X_train,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one hot encoding for Gender and City_Category features\n",
    "onehot_cols = ['Gender','City_Category']\n",
    "ce_one_hot = ce.OneHotEncoder(cols = onehot_cols)\n",
    "X_train1 = ce_one_hot.fit_transform(X_train1)\n",
    "X_test1 = ce_one_hot.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train1)\n",
    "X_test_scaled = scaler.transform(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBRegressor(colsample_bytree=0.8,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.07,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1,\n",
    "                 n_estimators=15000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42)\n",
    "model.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "#Evaluate\n",
    "print (model.score(X_train_scaled, y_train))\n",
    "print (model.score(X_test_scaled, y_test))\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "print(r2_score(predictions, y_test))\n",
    "mse = mean_squared_error(predictions,y_test)\n",
    "print(np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,1)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgboost.XGBRegressor(learning_rate =0.1, n_estimators=1000,\n",
    "                                                         gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                         nthread=4, scale_pos_weight=1, seed=27), \n",
    "param_grid = param_test1, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(X_train_scaled,y_train)\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'n_estimators':range(100,20000,200)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgboost.XGBRegressor(learning_rate =0.1,max_depth=3, min_child_weight=1,\n",
    "                                                         gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                         nthread=4, scale_pos_weight=1, seed=27), \n",
    "param_grid = param_test1, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(X_train_scaled,y_train)\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster ProductID, UserID by Purchase mean and count\n",
    "def cluster(df,train, test, variable):\n",
    "    a = df.groupby(variable)['Purchase'].agg({'count','mean'}).reset_index()\n",
    "    count = variable + '_count'\n",
    "    mean = variable + '_mean'\n",
    "    cluster = variable + '_cluster'\n",
    "    a.columns =[variable,count,mean]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    b = scaler.fit_transform(a[[count,mean]])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=5)  \n",
    "    kmeans.fit(b)\n",
    "\n",
    "    b_df = pd.DataFrame(b)\n",
    "\n",
    "    b_df['cluster'] = kmeans.labels_\n",
    "    b_df = pd.concat([b_df.reset_index(drop=True), \n",
    "                      a[[variable]].reset_index(drop=True)],\n",
    "                     axis=1,ignore_index= True)\n",
    "\n",
    "    b_df.columns = [count,mean,cluster, variable]\n",
    "\n",
    "    train_result = train.merge(b_df[[variable, cluster]],\n",
    "                           on=variable, how='left')\n",
    "    test_result = test.merge(b_df[[variable, cluster]],\n",
    "                           on=variable, how='left')\n",
    "    return train_result, test_result\n",
    "\n",
    "X_train1, X_test1 = cluster(df,df,df_test,'Product_ID')\n",
    "X_train1, X_test1 = cluster(df,X_train1,X_test1,'User_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_stat, df):\n",
    "    #Compute counts for high cardinality artributes\n",
    "    Occ_stats = df_stat.groupby('Occupation')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Occ_stats.columns = ['Occupation','Occ_count','Occ_mean','Occ_std']\n",
    "\n",
    "    Cat1_stats = df_stat.groupby('Product_Category_1')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat1_stats.columns = ['Product_Category_1','Cat1_count','Cat1_mean','Cat1_std']\n",
    "\n",
    "    Cat2_stats = df_stat.groupby('Product_Category_2')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat2_stats.columns = ['Product_Category_2','Cat2_count','Cat2_mean','Cat2_std']\n",
    "\n",
    "    Cat3_stats = df_stat.groupby('Product_Category_3')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    Cat3_stats.columns = ['Product_Category_3','Cat3_count','Cat3_mean','Cat3_std']\n",
    "    \n",
    "    ProdID_stats = df_stat.groupby('Product_ID')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    ProdID_stats.columns = ['Product_ID', 'ProdID_count','ProdID_mean','ProdID_std']\n",
    "    \n",
    "    UserID_stats = df_stat.groupby('User_ID')['Purchase'].agg(['count','mean','std']).reset_index()\n",
    "    UserID_stats.columns = ['User_ID', 'UserID_count','UserID_mean','UserID_std']\n",
    "\n",
    "    df1 = df.merge(ProdID_stats,on='Product_ID', how='left')\\\n",
    "                    .merge(UserID_stats,on='User_ID', how='left')\\\n",
    "                    .merge(Occ_stats,on='Occupation', how='left')\\\n",
    "                    .merge(Cat1_stats,on='Product_Category_1', how='left')\\\n",
    "                    .merge(Cat2_stats,on='Product_Category_2', how='left')\\\n",
    "                    .merge(Cat3_stats,on='Product_Category_3', how='left')\n",
    "\n",
    "\n",
    "#    #Taking care of missing data\n",
    "#     cols = ['Occ_count','Cat1_count',\n",
    "#             'Cat2_count','Cat3_count']\n",
    "#     from sklearn.preprocessing import Imputer\n",
    "#     imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "#     imputer = imputer.fit(df1[cols])\n",
    "#     df1[cols] = imputer.transform(df1[cols])\n",
    "#     df1[cols] = df1[cols].fillna(0)\n",
    "    df1.fillna(0,inplace=True)\n",
    "    \n",
    "    \n",
    "    df1['Stay_In_Current_City_Years'] = df1['Stay_In_Current_City_Years'].str.strip(\"+\")\n",
    "    df1['Stay_In_Current_City_Years'] = pd.to_numeric(df1['Stay_In_Current_City_Years'])\n",
    "    df1.loc[df1['Stay_In_Current_City_Years'] == 4, \"Stay_In_Current_City_Years\"] = 10\n",
    "\n",
    "    \n",
    "    #transform age\n",
    "    age_map = { '0-17' : 17, \n",
    "                  '18-25' : 25, \n",
    "                  '26-35' : 35, \n",
    "                  '36-45' : 45, \n",
    "                  '46-50' : 50,\n",
    "                  '51-55' : 55,\n",
    "                  '55+' : 70}\n",
    "\n",
    "    df1['Age_mapped'] = df1['Age'].map(age_map)\n",
    "    \n",
    "    df1 = df1.drop(['Age','Occupation','Product_Category_1',\n",
    "              'Product_Category_2','Product_Category_3',\n",
    "            'Product_ID','User_ID'], axis=1)\n",
    "\n",
    "    return df1\n",
    "\n",
    "# X_train1 = clean(df,X_train1)\n",
    "X_test1 = clean(df,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode cat features\n",
    "onehot_cols = ['Gender','City_Category']\n",
    "X_test1 = ce_one_hot.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "X_test_scaled = scaler.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Purchase'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_test[['User_ID','Product_ID','Purchase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('/Users/dangloan/Documents/learning_analytics/project/3_data/black_friday/sample8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importances\n",
    "importances = model.feature_importances_\n",
    "# importances = model1.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [X_test1.columns[i] for i in indices]\n",
    "\n",
    "\n",
    "a=list(zip(names,importances[indices]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(X_test1.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(X_test1.shape[1]), names, rotation=75)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
